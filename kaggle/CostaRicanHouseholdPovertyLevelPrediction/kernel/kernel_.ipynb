{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../input/train.csv\")\n",
    "test_data = pd.read_csv(\"../input/test.csv\")\n",
    "sub_orig = pd.read_csv(\"../input/sample_submission.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train_data.pop('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.drop(['Id'], axis=1, inplace=True)\n",
    "test_id = test_data.pop('Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9557, 141)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23856, 141)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MissingValuesImputer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, impute_zero_columns):\n",
    "        self.impute_zero_columns = impute_zero_columns\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        print(\"Mean Values Imputer\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        # Fill missing values for v18q1, v2a1 and rez_esc\n",
    "        for column in self.impute_zero_columns:\n",
    "            X[column] = X[column].fillna(0)\n",
    "\n",
    "        # For meaneduc we use the average schooling of household adults\n",
    "        self.X_with_meaneduc_na = X[pd.isnull(X['meaneduc'])]\n",
    "        self.mean_escolari_dict = dict(self.X_with_meaneduc_na.groupby('idhogar')['escolari'].apply(np.mean))\n",
    "        for row_index in self.X_with_meaneduc_na.index:\n",
    "            row_idhogar = X.at[row_index, 'idhogar']\n",
    "            X.at[row_index, 'meaneduc'] = self.mean_escolari_dict[row_idhogar]\n",
    "            X.at[row_index, 'SQBmeaned'] = np.square(self.mean_escolari_dict[row_idhogar])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RemoveObjectTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.target = ['dependency']\n",
    "        self.source = ['SQBdependency']\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        print(\"Remove Object Imputer\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        for i in range(0, len(self.target)):\n",
    "            X[self.target[i]] = np.sqrt(X[self.source[i]])\n",
    "            X.drop(self.source, axis=1, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_edu(row):\n",
    "    if (row['edjefe'] == 'yes' and row['edjefa'] == 'no') or (row['edjefe'] == 'no' and row['edjefa'] == 'yes'):\n",
    "        return 1\n",
    "    if row['edjefe'] == 'no' and row['edjefa'] == 'no':\n",
    "        return 0\n",
    "    if row['edjefe'] == 'yes' or row['edjefe'] == 'no':\n",
    "        return pd.to_numeric(row['edjefa'])\n",
    "    return pd.to_numeric(row['edjefe'])\n",
    "\n",
    "\n",
    "class CategoricalVariableTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        print(\"Categorical Variables Transformer\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X['house_holder_edu'] = X.apply(calculate_edu, axis=1).values\n",
    "        X.drop(['edjefe', 'edjefa'], axis=1, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnnecessaryColumnsRemoverTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, axis = 1):\n",
    "        print(\"Unnecessary Columns Remover Transformer\")\n",
    "        self.axis = axis\n",
    "        self.unnecessary_columns = [\n",
    "            'r4t3', 'tamhog', 'tamviv', 'hogar_total', 'v18q', 'v14a', 'agesq',\n",
    "            'mobilephone', 'energcocinar1', 'sanitario6',\n",
    "            'estadocivil7', 'lugar1', 'area1', 'female'\n",
    "        ]\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        unnecessary_columns_to_extend = [\n",
    "            [col for col in X.columns.tolist() if 'SQB' in col],\n",
    "            [col for col in X.columns.tolist() if 'epared' in col],\n",
    "            [col for col in X.columns.tolist() if 'etecho' in col],\n",
    "            [col for col in X.columns.tolist() if 'eviv' in col],\n",
    "            [col for col in X.columns.tolist() if 'instlevel' in col],\n",
    "            [col for col in X.columns.tolist() if 'pared' in col],\n",
    "            [col for col in X.columns.tolist() if 'piso' in col],\n",
    "            [col for col in X.columns.tolist() if 'techo' in col],\n",
    "            [col for col in X.columns.tolist() if 'abastagua' in col],\n",
    "            [col for col in X.columns.tolist() if 'elimbasu' in col],\n",
    "            [col for col in X.columns.tolist() if 'tipoviv' in col]\n",
    "        ]\n",
    "        \n",
    "        for col_list in unnecessary_columns_to_extend:\n",
    "            self.unnecessary_columns.extend(col_list)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X = X.drop(self.unnecessary_columns, axis = self.axis)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, axis = 1):\n",
    "        self.axis = axis\n",
    "        \n",
    "        # individual level boolean features\n",
    "        self.individual_boolean_features = [\n",
    "            'dis', 'male', 'estadocivil1', 'estadocivil2',\n",
    "            'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', \n",
    "            'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8',  \n",
    "            'parentesco9', 'parentesco10', 'parentesco11'\n",
    "        ]\n",
    "\n",
    "        # individual level ordered features\n",
    "        self.individual_ordered_features = ['escolari', 'age']\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        print(\"Feature Engineering Transformer\")\n",
    "        self.more_columns_to_drop = [\n",
    "            [col for col in X.columns.tolist() if 'parentesco' in col and 'parentesco1' not in col],\n",
    "            ['idhogar']\n",
    "        ]\n",
    "        \n",
    "        f = lambda x: x.std(ddof = 0)\n",
    "        f.__name__ = 'std_0'\n",
    "        self.aggregate_features = (['mean', 'max', 'min', 'sum', f])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        # Rooms\n",
    "        X['rent_per_room'] = X['v2a1' ] / X['rooms']\n",
    "        X['adults_per_room'] = X['hogar_adul'] / X['rooms']\n",
    "        X['males_per_room'] = X['r4h3'] / X['rooms']\n",
    "        X['females_per_room'] = X['r4m3'] / X['rooms']\n",
    "        X['children_per_room'] = X['hogar_nin'] / X['rooms']\n",
    "        X['humans_per_room'] = X['hhsize'] / X['rooms']\n",
    "        X['beds_per_room'] = X['bedrooms'] / X['rooms']\n",
    "        \n",
    "        # Bedroom\n",
    "        X['adults_per_bedroom'] = X['hogar_adul'] / X['bedrooms']\n",
    "        X['males_per_bedroom'] = X['r4h3'] / X['bedrooms']\n",
    "        X['females_per_bedroom'] = X['r4m3'] / X['bedrooms']\n",
    "        X['children_per_bedroom'] = X['hogar_nin'] / X['bedrooms']\n",
    "        X['humans_per_bedroom'] = X['hhsize'] / X['bedrooms']\n",
    "        \n",
    "        X['persons12less_fraction'] = (X['r4h1'] + X['r4m1']) / X['hhsize']\n",
    "        X['males12plus_fraction'] = X['r4h2'] / X['hhsize']\n",
    "        X['total_males_fraction'] = X['r4h3'] / X['hhsize']\n",
    "        X['females12plus_fraction'] = X['r4m2'] / X['hhsize']\n",
    "        X['all_females_fraction'] = X['r4m3'] / X['hhsize']\n",
    "        X['rent_per_person'] = X['v2a1'] / X['hhsize']\n",
    "        X['mobiles_per_person'] = X['qmobilephone'] / X['hhsize']\n",
    "        X['tablets_per_person'] = X['v18q1'] / X['hhsize']\n",
    "        X['mobiles_per_male'] = X['qmobilephone'] / X['r4h3']\n",
    "        X['tablets_per_male'] = X['v18q1'] / X['r4h3']\n",
    "        \n",
    "        # Create individual-level features\n",
    "        grouped_df = X.groupby('idhogar')[self.individual_boolean_features + self.individual_ordered_features]\n",
    "        grouped_df = grouped_df.agg(self.aggregate_features)\n",
    "        X = X.join(grouped_df, on = 'idhogar')\n",
    "        \n",
    "        # Finally remove the other parentesco columns since we are only going to use only heads of\n",
    "        # households for our scoring\n",
    "        for col in self.more_columns_to_drop:\n",
    "            X = X.drop(col, axis = self.axis) \n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LGBClassifierCV(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, axis = 0, lgb_params = None, fit_params = None, cv = 3, perform_random_search = False, use_train_test_split = False, use_kfold_split = True):\n",
    "        self.axis = axis\n",
    "        self.lgb_params = lgb_params\n",
    "        self.fit_params = fit_params\n",
    "        self.cv = cv\n",
    "        self.perform_random_search = perform_random_search\n",
    "        self.use_train_test_split = use_train_test_split\n",
    "        self.use_kfold_split = use_kfold_split\n",
    "    \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        feature_importances = []\n",
    "        for estimator in self.estimators_:\n",
    "            feature_importances.append(\n",
    "                estimator.feature_importances_\n",
    "            )\n",
    "        return np.mean(feature_importances, axis = 0)\n",
    "    \n",
    "    @property\n",
    "    def evals_result_(self):\n",
    "        evals_result = []\n",
    "        for estimator in self.estimators_:\n",
    "            evals_result.append(\n",
    "                estimator.evals_result_\n",
    "            )\n",
    "        return np.array(evals_result)\n",
    "    \n",
    "    @property\n",
    "    def best_scores_(self):\n",
    "        best_scores = []\n",
    "        for estimator in self.estimators_:\n",
    "            best_scores.append(\n",
    "                estimator.best_score_['validation']['macroF1']\n",
    "            )\n",
    "        return np.array(best_scores)\n",
    "    \n",
    "    @property\n",
    "    def cv_scores_(self):\n",
    "        return self.best_scores_ \n",
    "    \n",
    "    @property\n",
    "    def cv_score_(self):\n",
    "        return np.mean(self.best_scores_)\n",
    "    \n",
    "    @property\n",
    "    def best_iterations_(self):\n",
    "        best_iterations = []\n",
    "        for estimator in self.estimators_:\n",
    "            best_iterations.append(\n",
    "                estimator.best_iteration_\n",
    "            )\n",
    "        return np.array(best_iterations)\n",
    "    \n",
    "    @property\n",
    "    def best_iteration_(self):\n",
    "        return np.round(np.mean(self.best_iterations_))\n",
    "\n",
    "    def find_best_params_(self, X, y):\n",
    "        \n",
    "        # Define a search space for the parameters\n",
    "        lgb_search_params = {\n",
    "                  'num_leaves': sp_randint(20, 100), \n",
    "                  'min_child_samples': sp_randint(40, 100), \n",
    "                  'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "                  'subsample': sp_uniform(loc = 0.75, scale = 0.25), \n",
    "                  'colsample_bytree': sp_uniform(loc = 0.8, scale = 0.15),\n",
    "                  'reg_alpha': [0, 1e-3, 1e-1, 1, 10, 50, 100],\n",
    "                  'reg_lambda': [0, 1e-3, 1e-1, 1, 10, 50, 100]\n",
    "            }\n",
    "\n",
    "        x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.10, random_state = 42, stratify = y)\n",
    "        F1_scorer = make_scorer(f1_score, greater_is_better = True, average = 'macro')\n",
    "\n",
    "        lgb_model = lgb.LGBMClassifier(**self.lgb_params)\n",
    "        self.fit_params[\"eval_set\"] = [(x_train, y_train), (x_val, y_val)]\n",
    "        self.fit_params[\"verbose\"] = 200\n",
    "\n",
    "        rs = RandomizedSearchCV(estimator = lgb_model, \n",
    "                                param_distributions = lgb_search_params, \n",
    "                                n_iter = 100,\n",
    "                                scoring = F1_scorer,\n",
    "                                cv = 5,\n",
    "                                refit = True,\n",
    "                                random_state = 314,\n",
    "                                verbose = False,\n",
    "                                fit_params = self.fit_params)\n",
    "        \n",
    "        # Fit the random search\n",
    "        _ = rs.fit(x_train, y_train)\n",
    "        \n",
    "        print(\"Optimal LGB parameters:\")\n",
    "        print(rs.best_params_)\n",
    "        with open(\"lgb_best_params.pickle\", \"wb\") as lgb_best_params:\n",
    "            pickle.dump(rs.best_params_, lgb_best_params)\n",
    "        \n",
    "        return rs.best_params_\n",
    "    \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        print(\"LGBClassifierCV\")\n",
    "        \n",
    "        # Use only heads of households for scoring\n",
    "        X.insert(0, 'Target', y)\n",
    "        X = X.query('parentesco1 == 1')\n",
    "        y = X['Target'] - 1\n",
    "        X = X.drop(['Target', 'parentesco1'], 1)\n",
    "        print(\"Number of columns in train - \" + str(X.shape[1]))\n",
    "        \n",
    "        self.estimators_ = []\n",
    "        \n",
    "        # Use the best parameters to fit a model to whole data\n",
    "        if self.perform_random_search:\n",
    "            self.lgb_optimal_params = self.find_best_params_(X, y)\n",
    "            \n",
    "        # Use a simple train-test split. I have found that this gives a better local CV score than\n",
    "        # K folds.\n",
    "        if self.use_train_test_split:\n",
    "            x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "            \n",
    "            lgb_model = lgb.LGBMClassifier(**self.lgb_params)\n",
    "            if self.perform_random_search:\n",
    "                lgb_model.set_params(**self.lgb_optimal_params)\n",
    "            \n",
    "            lgb_model.fit(\n",
    "                    x_train, y_train,\n",
    "                    eval_set = [(x_train, y_train), (x_val, y_val)],\n",
    "                    **self.fit_params\n",
    "            )\n",
    "            print(\"Train F1 - \" + str(lgb_model.best_score_['train']['macroF1']) + \"   \" + \"Validation F1 - \" + str(lgb_model.best_score_['validation']['macroF1']))\n",
    "            self.estimators_.append(lgb_model)\n",
    "            \n",
    "        # When not using random search to tune parameters, proceed with a simple Stratified Kfold CV\n",
    "        if self.use_kfold_split:\n",
    "            kf = StratifiedKFold(n_splits = self.cv, shuffle = True)\n",
    "            for fold_index, (train, valid) in enumerate(kf.split(X, y)):\n",
    "                print(\"Train Fold Index - \" + str(fold_index))\n",
    "\n",
    "                lgb_model = lgb.LGBMClassifier(**self.lgb_params)\n",
    "                if self.perform_random_search:\n",
    "                    lgb_model.set_params(**self.lgb_optimal_params)\n",
    "\n",
    "                lgb_model.fit(\n",
    "                        X.iloc[train], y.iloc[train],\n",
    "                        eval_set = [(X.iloc[train], y.iloc[train]), (X.iloc[valid], y.iloc[valid])],\n",
    "                        **self.fit_params\n",
    "                )\n",
    "                print(\"Train F1 - \" + str(lgb_model.best_score_['train']['macroF1']) + \"   \" + \"Validation F1 - \" + str(lgb_model.best_score_['validation']['macroF1']))\n",
    "\n",
    "                self.estimators_.append(lgb_model)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Remove this column since we are using only heads of households for scoring\n",
    "        X = X.drop('parentesco1', 1)\n",
    "        \n",
    "        # When not using random search, use voting to get predictions from all CV estimators.\n",
    "        y_pred = []\n",
    "        for estimator_index, estimator in enumerate(self.estimators_):\n",
    "            print(\"Estimator Index - \" + str(estimator_index))\n",
    "            y_pred.append(estimator.predict(X))\n",
    "        return np.mean(y_pred, axis = self.axis).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lgb_params():\n",
    "    \n",
    "    def evaluate_macroF1_lgb(truth, predictions):  \n",
    "        pred_labels = predictions.reshape(len(np.unique(truth)), -1).argmax(axis = 0)\n",
    "        f1 = f1_score(truth, pred_labels, average = 'macro')\n",
    "        return ('macroF1', f1, True)\n",
    "\n",
    "    def learning_rate_power_0997(current_iter):\n",
    "            base_learning_rate = 0.1\n",
    "            min_learning_rate = 0.02\n",
    "            lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "            return max(lr, min_learning_rate)\n",
    "\n",
    "    lgb_params = {'boosting_type': 'dart',\n",
    "                  'class_weight': 'balanced',\n",
    "                  \"objective\": 'multiclassova',\n",
    "                  'metric': None,\n",
    "                  'silent': True,\n",
    "                  'random_state': 0,\n",
    "                  'n_jobs': -1}\n",
    "\n",
    "    fit_params={\"early_stopping_rounds\": 400, \n",
    "                \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "                'eval_names': ['train', 'validation'],\n",
    "                'verbose': False,\n",
    "                'categorical_feature': 'auto'}\n",
    "    \n",
    "    return lgb_params, fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnecessary Columns Remover Transformer\n",
      "Mean Values Imputer\n",
      "Remove Object Imputer\n",
      "Categorical Variables Transformer\n",
      "Feature Engineering Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:551: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBClassifierCV\n",
      "Number of columns in train - 185\n",
      "Train F1 - 0.866419406663   Validation F1 - 0.45010643404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:551: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator Index - 0\n",
      "Local CV Score - 0.45010643404\n",
      "              Target\n",
      "Id                  \n",
      "ID_2f6873615       4\n",
      "ID_1c78846d2       4\n",
      "ID_e5442cf6a       4\n",
      "ID_a8db26a79       4\n",
      "ID_a62966799       4\n"
     ]
    }
   ],
   "source": [
    "lgb_params, lgb_fit_params = get_lgb_params()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('na_imputer', MissingValuesImputer(impute_zero_columns = ['v18q1', 'v2a1', 'rez_esc'])),\n",
    "    ('remove_imputer', RemoveObjectTransformer()),\n",
    "    ('cat_transformer', CategoricalVariableTransformer()),\n",
    "    ('unnecessary_columns_remover_transformer', UnnecessaryColumnsRemoverTransformer()),\n",
    "    ('feature_engineering_transformer', FeatureEngineeringTransformer()),\n",
    "    ('lgb', LGBClassifierCV(lgb_params = lgb_params,\n",
    "                            fit_params = lgb_fit_params,\n",
    "                            cv = 5,\n",
    "                            perform_random_search = False,\n",
    "                            use_train_test_split = True,\n",
    "                            use_kfold_split = False)\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(train_data.copy(), target.copy())\n",
    "pred = pipeline.predict(test_data.copy())\n",
    "print(\"Local CV Score - \" + str(pipeline.named_steps['lgb'].cv_score_))\n",
    "sub_orig['Target'] = pred + 1\n",
    "sub_orig.to_csv('Pipeline_Base_LGB_'+ str(pipeline.named_steps['lgb'].cv_score_) + '.csv')\n",
    "print(sub_orig.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
